{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld_PolicyIteration():\n",
    "    \n",
    "    def __init__(self,size,reward_value):\n",
    "        \n",
    "        self.size = size\n",
    "        self.reward_value = reward_value\n",
    "        \n",
    "    def InitializePolicy(self):\n",
    "        \n",
    "        coordinates = [(i,j) for i in range(1,self.size+1) for j in range(1,self.size+1)]\n",
    "        self.policy = {}\n",
    "        for i in coordinates:\n",
    "            self.policy[i]=[0.25,0.25,0.25,0.25]\n",
    "\n",
    "    def InitializeValueFunction(self):\n",
    "\n",
    "        self.large_Vk = np.zeros((self.size+2,self.size+2))       \n",
    "        \n",
    "    def Coordinates(self):\n",
    "        \n",
    "        cod = np.meshgrid(list(range(1,self.size+1)),\n",
    "                          list(range(1,self.size+1)))\n",
    "        self.row_cod = cod[1]\n",
    "        self.col_cod = cod[0]\n",
    "        \n",
    "    def Reward(self):\n",
    "        \n",
    "        if isinstance(self.reward_value,int): \n",
    "            self.reward_matrix = np.ones((self.size,self.size))*self.reward_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InitializePolicy(size):\n",
    "    \n",
    "    coordinates = [(i,j) for i in range(1,size+1) for j in range(1,size+1)]\n",
    "    policy1 = {}\n",
    "    for i in coordinates:\n",
    "        policy1[i]=[0.25,0.25,0.25,0.25]\n",
    "        \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 2\n",
    "isinstance(a,int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld_PolicyEvaluation():\n",
    "    \n",
    "    def __init__(self,size,reward,itera,policy):\n",
    "        \n",
    "        self.size = size\n",
    "        self.iter = itera\n",
    "        \n",
    "        # instant reward, represents the first part of bellman equation\n",
    "        # sum_{a}pi(a|s)*sum_{s',r}p(s',r|s,a)*r\n",
    "        # r is set to be reward\n",
    "        self.instant_r = reward*np.ones((self.size,self.size))\n",
    "        self.instant_r[0,0]=0\n",
    "        self.instant_r[-1,-1]=0\n",
    "        \n",
    "        self.policy = policy\n",
    "        \n",
    "    def __DataStruct(self):\n",
    "                \n",
    "        self.actions = ['up','down','left','right']\n",
    "        \n",
    "        # Initialize value-function matrix\n",
    "        self.large_Vk = np.ones((self.size+2,self.size+2))*(-1)\n",
    "        self.large_Vk[1,1]=0\n",
    "        self.large_Vk[self.size,self.size]=0\n",
    "        \n",
    "        # execute-action related arrays\n",
    "        self.cod = np.meshgrid(list(range(1,self.size+1)),\n",
    "                               list(range(1,self.size+1)))\n",
    "        \n",
    "        self.move_left = [self.cod[1],(self.cod[0]+self.actions_move['up']).astype(int)]\n",
    "        self.move_right = [self.cod[1],(self.cod[0]+self.actions_move['down']).astype(int)]\n",
    "        self.move_up = [(self.cod[1]+self.actions_move['left']).astype(int),self.cod[0]]\n",
    "        self.move_down = [(self.cod[1]+self.actions_move['right']).astype(int),self.cod[0]]\n",
    "        \n",
    "        self.actions_cod = {'left':self.move_left,\n",
    "                            'right':self.move_right,\n",
    "                            'up':self.move_up,\n",
    "                            'down':self.move_down}\n",
    "        \n",
    "    def Act_Update(self):\n",
    "        \n",
    "        self.Vk_dir = {}\n",
    "        sum_temp_vk = 0                \n",
    "        for a in self.actions:\n",
    "            \n",
    "            # self.Vk_dir not used after defination. This is for debugging.\n",
    "            self.Vk_dir[a] = self.large_Vk[self.actions_cod[a][0],self.actions_cod[a][1]]\n",
    "            sum_temp_vk+=self.policy[a]*self.Vk_dir[a]\n",
    "            \n",
    "        sum_temp_vk+=self.instant_r\n",
    "        self.large_Vk[self.cod[1],self.cod[0]] = sum_temp_vk\n",
    "        self.large_Vk[1,1]=0\n",
    "        self.large_Vk[self.size,self.size]=0\n",
    "        \n",
    "        self.large_Vk[0,:] = self.large_Vk[1,:]\n",
    "        self.large_Vk[:,-1] = self.large_Vk[:,-2]\n",
    "        self.large_Vk[-1,:] = self.large_Vk[-2,:]\n",
    "        self.large_Vk[:,0] = self.large_Vk[:,1]\n",
    "        \n",
    "    def ReadyPlayer1(self):\n",
    "        \n",
    "        self.__DataStruct()\n",
    "        \n",
    "        for i in range(self.iter):\n",
    "            \n",
    "            self.Act_Update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy_Iteration():\n",
    "    \n",
    "    def __init__(self,S,A):\n",
    "        \n",
    "        self.states = S\n",
    "        self.actions = A\n",
    "        \n",
    "    def Reward(self):\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def State_Trans_Prob(self):\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
