{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld_PolicyIteration():\n",
    "    \n",
    "    def __init__(self,size,reward_value,delta):\n",
    "        \n",
    "        self.size = size\n",
    "        self.reward_value = reward_value\n",
    "        self.actions = ['up','down','left','right']\n",
    "        self.n_action = len(self.actions)\n",
    "        self.delta = np.ones((size,size))*delta\n",
    "        \n",
    "    def InitializePolicy(self):\n",
    "        \n",
    "        # policy is a n_action x N x N ndarray.  \n",
    "        # Each NxN matrix is for the policy of specific action through all states\n",
    "        self.policy = np.ones((self.n_action,self.size,self.size))*0.25\n",
    "\n",
    "    def InitializeValueFunction(self):\n",
    "\n",
    "        # The size of value_function matrix is (N+2 x N+2), \n",
    "        # this is more efficient for the following code\n",
    "        self.large_Vk = np.zeros((self.size+2,self.size+2))    \n",
    "        \n",
    "    def Coordinates(self):\n",
    "        \n",
    "        # coordinates are used for indicating specific state and value_function given the state\n",
    "        cod = np.meshgrid(list(range(1,self.size+1)),\n",
    "                          list(range(1,self.size+1)))\n",
    "        self.row_cod = cod[1]\n",
    "        self.col_cod = cod[0]\n",
    "        \n",
    "        self.move_up = [self.row_cod,(self.col_cod-1).astype(int)]\n",
    "        self.move_down = [self.row_cod,(self.col_cod+1).astype(int)]\n",
    "        self.move_left = [(self.row_cod-1).astype(int),self.col_cod]\n",
    "        self.move_right = [(self.row_cod+1).astype(int),self.col_cod]\n",
    "        \n",
    "        self.move = {0:self.move_up,\n",
    "                     1:self.move_down,\n",
    "                     2:self.move_left,\n",
    "                     3:self.move_right}\n",
    "        \n",
    "    def Reward(self):\n",
    "        \n",
    "        # define r(s',a,s), which is also the instant reward, the reward in first term of Bellman Equation\n",
    "        if isinstance(self.reward_value,int): \n",
    "            self.reward_matrix = np.ones((self.size,self.size))*self.reward_value\n",
    "            self.reward_matrix[0,0] = 0\n",
    "            self.reward_matrix[-1,-1] = 0\n",
    "\n",
    "    def Policy_Eval(self):\n",
    "        \n",
    "        Vk_temp = 0\n",
    "        for i in range(self.n_action):\n",
    "            Vk_temp += self.policy[i]*self.large_Vk[self.move[i][0],self.move[i][1]]\n",
    "            \n",
    "        Vk_temp += self.reward_matrix\n",
    "        \n",
    "        self.large_Vk[self.row_cod,self.col_cod] = Vk_temp\n",
    "        self.large_Vk[1,1]=0\n",
    "        self.large_Vk[self.size,self.size]=0\n",
    "        \n",
    "        self.large_Vk[0,:] = self.large_Vk[1,:]\n",
    "        self.large_Vk[:,-1] = self.large_Vk[:,-2]\n",
    "        self.large_Vk[-1,:] = self.large_Vk[-2,:]\n",
    "        self.large_Vk[:,0] = self.large_Vk[:,1]\n",
    "        \n",
    "    def Policy_Impr(self):\n",
    "        \n",
    "        action_state_value = []\n",
    "        for i in self.move.keys():\n",
    "            action_state_value.append(self.large_Vk[self.move[i][0],self.move[i][1]])\n",
    "        action_state_value = np.array(action_state_value)\n",
    "        best_action = np.argmax(action_state_value,axis=0)\n",
    "        \n",
    "        for i in range(self.n_action):\n",
    "            self.policy[i] = 1*((self.policy[i]*(best_action==i))!=0)\n",
    "    \n",
    "    def ReadyPlayer1(self):\n",
    "        \n",
    "        self.InitializePolicy()\n",
    "        self.InitializeValueFunction()\n",
    "        self.Coordinates()\n",
    "        self.Reward()\n",
    "        \n",
    "        this_Vk = self.large_Vk[self.row_cod,self.col_cod]\n",
    "        last_Vk = np.ones((self.size,self.size))*1000\n",
    "        \n",
    "        last_policy = self.policy-1\n",
    "        this_policy = self.policy\n",
    "        while (last_policy==this_policy).all()==False:\n",
    "            \n",
    "            while (np.abs(this_Vk - last_Vk)<self.delta).all()==False:\n",
    "                last_Vk=this_Vk.copy()\n",
    "                self.Policy_Eval()\n",
    "                this_Vk = self.large_Vk[self.row_cod,self.col_cod]\n",
    "                \n",
    "            last_policy=this_policy.copy()\n",
    "            self.Policy_Impr()\n",
    "            this_policy=self.policy\n",
    "        \n",
    "        self.policy[:,0,0]=0\n",
    "        self.policy[:,-1,-1]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "GridWorld = GridWorld_PolicyIteration(4,-1,0.0000000001)\n",
    "GridWorld.ReadyPlayer1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -14., -20., -22.],\n",
       "       [-14., -18., -20., -20.],\n",
       "       [-20., -20., -18., -14.],\n",
       "       [-22., -20., -14.,   0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GridWorld.large_Vk[GridWorld.row_cod,GridWorld.col_cod]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 1., 1.],\n",
       "        [0., 1., 1., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 1., 1., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GridWorld.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
