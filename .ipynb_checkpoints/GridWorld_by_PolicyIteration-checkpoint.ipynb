{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld_PolicyIteration():\n",
    "    \n",
    "    def __init__(self,size,reward_value,delta):\n",
    "        \n",
    "        self.size = size\n",
    "        self.reward_value = reward_value\n",
    "        self.actions = ['up','down','left','right']\n",
    "        self.n_action = len(self.actions)\n",
    "        self.delta = np.ones((size,size))*delta\n",
    "        \n",
    "    def InitializePolicy(self):\n",
    "        \n",
    "        # policy is a n_action x N x N ndarray.  \n",
    "        # Each NxN matrix is for the policy of specific action through all states\n",
    "        self.policy = np.ones((self.n_action,self.size,self.size))*0.25\n",
    "\n",
    "    def InitializeValueFunction(self):\n",
    "\n",
    "        # The size of value_function matrix is (N+2 x N+2), \n",
    "        # this is more efficient for the following code\n",
    "        self.large_Vk = np.zeros((self.size+2,self.size+2))    \n",
    "        \n",
    "    def Coordinates(self):\n",
    "        \n",
    "        # coordinates are used for indicating specific state and value_function given the state\n",
    "        cod = np.meshgrid(list(range(1,self.size+1)),\n",
    "                          list(range(1,self.size+1)))\n",
    "        self.row_cod = cod[1]\n",
    "        self.col_cod = cod[0]\n",
    "        \n",
    "        self.move_up = [self.row_cod,(self.col_cod-1).astype(int)]\n",
    "        self.move_down = [self.row_cod,(self.col_cod+1).astype(int)]\n",
    "        self.move_left = [(self.row_cod-1).astype(int),self.col_cod]\n",
    "        self.move_right = [(self.row_cod+1).astype(int),self.col_cod]\n",
    "        \n",
    "        self.move = {0:self.move_up,\n",
    "                     1:self.move_down,\n",
    "                     2:self.move_left,\n",
    "                     3:self.move_right}\n",
    "        \n",
    "    def Reward(self):\n",
    "        \n",
    "        # define r(s',a,s), which is also the instant reward, the reward in first term of Bellman Equation\n",
    "        if isinstance(self.reward_value,int): \n",
    "            self.reward_matrix = np.ones((self.size,self.size))*self.reward_value\n",
    "            self.reward_matrix[0,0] = 0\n",
    "            self.reward_matrix[-1,-1] = 0\n",
    "\n",
    "    def Policy_Eval(self):\n",
    "        \n",
    "        Vk_temp = 0\n",
    "        for i in range(self.n_action):\n",
    "            Vk_temp += self.policy[i]*self.large_Vk[self.move[i][0],self.move[i][1]]\n",
    "            \n",
    "        Vk_temp += self.reward_matrix\n",
    "        \n",
    "        self.large_Vk[self.row_cod,self.col_cod] = Vk_temp\n",
    "        self.large_Vk[1,1]=0\n",
    "        self.large_Vk[self.size,self.size]=0\n",
    "        \n",
    "        self.large_Vk[0,:] = self.large_Vk[1,:]\n",
    "        self.large_Vk[:,-1] = self.large_Vk[:,-2]\n",
    "        self.large_Vk[-1,:] = self.large_Vk[-2,:]\n",
    "        self.large_Vk[:,0] = self.large_Vk[:,1]\n",
    "        \n",
    "    def Policy_Impr(self):\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def ReadyPlayer1(self):\n",
    "        \n",
    "        self.InitializePolicy()\n",
    "        self.InitializeValueFunction()\n",
    "        self.Coordinates()\n",
    "        self.Reward()\n",
    "        \n",
    "        this_Vk = self.large_Vk[self.row_cod,self.col_cod]\n",
    "        last_Vk = np.ones((self.size,self.size))*1000\n",
    "        \n",
    "        while (np.abs(this_Vk - last_Vk)<self.delta).all()==False:\n",
    "            last_Vk=this_Vk.copy()\n",
    "            self.Policy_Eval()\n",
    "            this_Vk = self.large_Vk[self.row_cod,self.col_cod]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InitializePolicy(size):\n",
    "    \n",
    "    coordinates = [(i,j) for i in range(1,size+1) for j in range(1,size+1)]\n",
    "    policy1 = {}\n",
    "    for i in coordinates:\n",
    "        policy1[i]=[0.25,0.25,0.25,0.25]\n",
    "        \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 2\n",
    "isinstance(a,int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld_PolicyEvaluation():\n",
    "    \n",
    "    def __init__(self,size,reward,itera,policy):\n",
    "        \n",
    "        self.size = size\n",
    "        self.iter = itera\n",
    "        \n",
    "        # instant reward, represents the first part of bellman equation\n",
    "        # sum_{a}pi(a|s)*sum_{s',r}p(s',r|s,a)*r\n",
    "        # r is set to be reward\n",
    "        self.instant_r = reward*np.ones((self.size,self.size))\n",
    "        self.instant_r[0,0]=0\n",
    "        self.instant_r[-1,-1]=0\n",
    "        \n",
    "        self.policy = policy\n",
    "        \n",
    "    def __DataStruct(self):\n",
    "                \n",
    "        self.actions = ['up','down','left','right']\n",
    "        \n",
    "        # Initialize value-function matrix\n",
    "        self.large_Vk = np.ones((self.size+2,self.size+2))*(-1)\n",
    "        self.large_Vk[1,1]=0\n",
    "        self.large_Vk[self.size,self.size]=0\n",
    "        \n",
    "        # execute-action related arrays\n",
    "        self.cod = np.meshgrid(list(range(1,self.size+1)),\n",
    "                               list(range(1,self.size+1)))\n",
    "        \n",
    "        self.move_left = [self.cod[1],(self.cod[0]+self.actions_move['up']).astype(int)]\n",
    "        self.move_right = [self.cod[1],(self.cod[0]+self.actions_move['down']).astype(int)]\n",
    "        self.move_up = [(self.cod[1]+self.actions_move['left']).astype(int),self.cod[0]]\n",
    "        self.move_down = [(self.cod[1]+self.actions_move['right']).astype(int),self.cod[0]]\n",
    "        \n",
    "        self.actions_cod = {'left':self.move_left,\n",
    "                            'right':self.move_right,\n",
    "                            'up':self.move_up,\n",
    "                            'down':self.move_down}\n",
    "        \n",
    "    def Act_Update(self):\n",
    "        \n",
    "        self.Vk_dir = {}\n",
    "        sum_temp_vk = 0                \n",
    "        for a in self.actions:\n",
    "            \n",
    "            # self.Vk_dir not used after defination. This is for debugging.\n",
    "            self.Vk_dir[a] = self.large_Vk[self.actions_cod[a][0],self.actions_cod[a][1]]\n",
    "            sum_temp_vk+=self.policy[a]*self.Vk_dir[a]\n",
    "            \n",
    "        sum_temp_vk+=self.instant_r\n",
    "        self.large_Vk[self.cod[1],self.cod[0]] = sum_temp_vk\n",
    "        self.large_Vk[1,1]=0\n",
    "        self.large_Vk[self.size,self.size]=0\n",
    "        \n",
    "        self.large_Vk[0,:] = self.large_Vk[1,:]\n",
    "        self.large_Vk[:,-1] = self.large_Vk[:,-2]\n",
    "        self.large_Vk[-1,:] = self.large_Vk[-2,:]\n",
    "        self.large_Vk[:,0] = self.large_Vk[:,1]\n",
    "        \n",
    "    def ReadyPlayer1(self):\n",
    "        \n",
    "        self.__DataStruct()\n",
    "        \n",
    "        for i in range(self.iter):\n",
    "            \n",
    "            self.Act_Update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy_Iteration():\n",
    "    \n",
    "    def __init__(self,S,A):\n",
    "        \n",
    "        self.states = S\n",
    "        self.actions = A\n",
    "        \n",
    "    def Reward(self):\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def State_Trans_Prob(self):\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
